{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpGcVO-GJlZA"
      },
      "source": [
        "# **Tokenization in Natural Language Processing**\n",
        "\n",
        "## **Tokenization**\n",
        "\n",
        "Tokenization is splitting the large chunk of word, sentence, document into smaller unit (single word or combination of words). Smaller units are known as tokens.\n",
        "\n",
        ">  <img src=\"https://curator-production.s3.us.cloud-object-storage.appdomain.cloud/uploads/course-v1:IBMSkillsNetwork+GPXX0A7BEN+v1.jpg\" height=\"200\" width=\"400\" align=\"centre\">\n",
        "\n",
        "**Why we need tokenization?**\n",
        "\n",
        "Tokenization is essential because machines can only process numerical data. To enable machines to comprehend raw text, we must first divide it into individual words. These words are then typically encoded into numeric formats, such as using the Bag of Words model or the TF-IDF method. In essence, tokenization is a crucial step in transforming raw text into a format that machines can understand.\n",
        "\n",
        "In the NLP pipeline, tokenization is the initial step of data processing. It facilitates the subsequent analysis of textual data by extracting useful features from the text.\n",
        "\n",
        "## **Types of Tokenizers in NLP**\n",
        "\n",
        "Different types of tokenizers are used depending on the specific scenario. For instance, if we are developing an NLP-based phishing email detector, we first need to break down the email content into words using a word tokenizer. Similarly, if we want to analyze a paragraph sentence by sentence, we would use a sentence tokenizer.\n",
        "\n",
        "The NLTK library in Python provides several types of tokenizers, including:\n",
        "\n",
        "\n",
        "*   Word Tokenizer\n",
        "*   Sentence Tokenizer\n",
        "*   Tweet Tokenizer\n",
        "*   Regex Tokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Word Tokenizer**\n",
        "\n",
        "A word tokenizer splits text into individual words. In Python, this can be achieved using the split() method, which divides the text based on whitespace by default. This method, known as whitespace tokenization, often falls short as it fails to correctly handle contraction words like \"can't,\" \"hasn't,\" and \"wouldn't.\" Using the NLTK-based word tokenizer is more effective as it can handle contractions and words like \"o'clock\" that are not contractions.\n"
      ],
      "metadata": {
        "id": "aDOjVUYXJHJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = '''At five o'clock in the morning I went to railway station near by my home.\n",
        "              I'll never go to that railway station again.\n",
        "              '''\n",
        "print(document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xdd-QhWJMLW",
        "outputId": "15bf67e5-9898-47ee-e721-d620de1b0217"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At five o'clock in the morning I went to railway station near by my home.\n",
            "              I'll never go to that railway station again.\n",
            "              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whitespace tokenization"
      ],
      "metadata": {
        "id": "oX5IL9KpJVaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(document.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2hh5C57JSfx",
        "outputId": "8b0ab210-e82e-4e15-b0d5-f2dfcfdf6a1d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['At', 'five', \"o'clock\", 'in', 'the', 'morning', 'I', 'went', 'to', 'railway', 'station', 'near', 'by', 'my', 'home.', \"I'll\", 'never', 'go', 'to', 'that', 'railway', 'station', 'again.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK word tokenizer"
      ],
      "metadata": {
        "id": "XSBznYEFJaq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "words = word_tokenize(document)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v9nVzoIJmUn",
        "outputId": "b5ccf961-ec24-4103-e179-4d42d5eaebba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['At', 'five', \"o'clock\", 'in', 'the', 'morning', 'I', 'went', 'to', 'railway', 'station', 'near', 'by', 'my', 'home', '.', 'I', \"'ll\", 'never', 'go', 'to', 'that', 'railway', 'station', 'again', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the above code the whitespace tokenizer is unable to identify the contraction word ‚ÄúI‚Äôll‚Äù and also concatenated ‚Äú.‚Äù with the words ‚Äòhome‚Äô and ‚Äòagain‚Äô. On the other hand, NLTK‚Äôs word tokenizer not only breaks on whitespaces but also breaks contraction words such as I‚Äôll into ‚ÄúI‚Äù and ‚Äú‚Äòll‚Äù as well as it doesn‚Äôt break ‚Äúo‚Äôclock‚Äù and treats it as a separate token."
      ],
      "metadata": {
        "id": "IWDN7bIBKMhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sentence Tokenizer**\n",
        "\n",
        "Tokenising based on a sentence requires us to split based on the period (‚Äò.‚Äô). Let‚Äôs have a look at the NLTK sentence tokenizer in the below code."
      ],
      "metadata": {
        "id": "XQunGPjBKPjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = '''At five o'clock in the morning I went to railway station near by my home.\n",
        "              I'll never go to that railway station again.\n",
        "              '''\n",
        "print(document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ze73DximKZBi",
        "outputId": "76826e9e-3d3b-4f7e-acc4-2a41118eee5c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At five o'clock in the morning I went to railway station near by my home.\n",
            "              I'll never go to that railway station again.\n",
            "              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK sentence tokenizer"
      ],
      "metadata": {
        "id": "GpDhdfm8KcLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = sent_tokenize(document)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBPlW8SmKbOb",
        "outputId": "128f0195-b894-47dd-a86c-8036accf2edc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"At five o'clock in the morning I went to railway station near by my home.\", \"I'll never go to that railway station again.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tweet Tokenizer**\n",
        "\n",
        "A problem with word tokenizer is that it fails to tokenize emojis and other complex special characters such as words with hashtags. Emojis are common these days and people use them all the time."
      ],
      "metadata": {
        "id": "GA-QRRc1Keot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "msg=\"WIN with PRIMI to celebrate #NationalPizzaDay! RT this tweet, tell us what your fav PRIMI PIZZA is & stand to WIN a R250 voucher. This is gr8 <3  ü•≥üçï\"\n",
        "print(msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iynNNdltKnJ3",
        "outputId": "a5f4f0c6-ae43-4981-a5b9-52c5ddf6aa58"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WIN with PRIMI to celebrate #NationalPizzaDay! RT this tweet, tell us what your fav PRIMI PIZZA is & stand to WIN a R250 voucher. This is gr8 <3  ü•≥üçï\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(msg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz-woVrdKpBM",
        "outputId": "79817e1e-05f0-4879-f8ae-da15f70200cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['WIN', 'with', 'PRIMI', 'to', 'celebrate', '#', 'NationalPizzaDay', '!', 'RT', 'this', 'tweet', ',', 'tell', 'us', 'what', 'your', 'fav', 'PRIMI', 'PIZZA', 'is', '&', 'stand', 'to', 'WIN', 'a', 'R250', 'voucher', '.', 'This', 'is', 'gr8', '<', '3', 'ü•≥üçï']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "tknzr.tokenize(msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7irfq6EKr-i",
        "outputId": "f657c2ed-4477-45c3-f38b-27d8c2ec0175"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['WIN',\n",
              " 'with',\n",
              " 'PRIMI',\n",
              " 'to',\n",
              " 'celebrate',\n",
              " '#NationalPizzaDay',\n",
              " '!',\n",
              " 'RT',\n",
              " 'this',\n",
              " 'tweet',\n",
              " ',',\n",
              " 'tell',\n",
              " 'us',\n",
              " 'what',\n",
              " 'your',\n",
              " 'fav',\n",
              " 'PRIMI',\n",
              " 'PIZZA',\n",
              " 'is',\n",
              " '&',\n",
              " 'stand',\n",
              " 'to',\n",
              " 'WIN',\n",
              " 'a',\n",
              " 'R250',\n",
              " 'voucher',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'gr8',\n",
              " '<3',\n",
              " 'ü•≥',\n",
              " 'üçï']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the above code, the word tokenizer breaks the emoji '<3' into ‚Äò<‚Äò and ‚Äò3‚Äô which is unacceptable in the domain of tweets language.\n",
        "\n",
        "Emojis have their own significance in areas like sentiment analysis where a happy face and sad face can alone prove to be a really good predictor of sentiment. Similarly, the hashtags are broken into two tokens. A hashtag is used for searching for specific topics or photos on social media apps such as Instagram and Facebook. So there, you want to use the hashtag as is.\n",
        "\n",
        "But tweet tokenizer handles all the emojis and the hashtags pretty well."
      ],
      "metadata": {
        "id": "epul0P-0KvNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regex Tokenizer**\n",
        "\n",
        "Regex tokenizer takes a regular expression and tokenizes and returns results based on the pattern specified in the regular expression.\n",
        "\n",
        "Now, let‚Äôs say we want to tokenize the tweet message based on hashtags. Then let‚Äôs look at the below code to understand how to use the regex tokenizer."
      ],
      "metadata": {
        "id": "nuuJ1XZUM-JK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "message = \"WIN with PRIMI to celebrate #NationalPizzaDay! RT this tweet, tell us what your fav PRIMI PIZZA is & stand to WIN a R250 voucher. This is gr8 <3  ü•≥üçï\"\n",
        "pattern = \"#[\\w]+\""
      ],
      "metadata": {
        "id": "hteRX-7wKuS3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regexp_tokenize(message, pattern)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QpX2zRBNM4l",
        "outputId": "aa02f6cc-b1b7-4dbd-aa8a-723d3dd8230b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#NationalPizzaDay']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the above code the regex tokenizer successfully tokenizes the tweet based on the hashtag ‚ÄòNationalPizzaDay‚Äô"
      ],
      "metadata": {
        "id": "cS1MKnSDNQmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yKVyw-ecAQwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Byte Pair Encoding (BPE) Algorithm**\n",
        "\n",
        "BPE was originally a data compression algorithm that you use to find the best way to represent data by identifying the common byte pairs. We now use it in NLP to find the best representation of text using the smallest number of tokens.\n",
        "\n",
        "Working process:\n",
        "\n",
        "* Append an identifier (</w>) to the end of each word to mark the word boundaries, then calculate the frequency of each word in the text.\n",
        "* Split the words into individual characters and compute the frequency of each character.\n",
        "* For a set number of iterations, identify the most frequent consecutive byte pairs among the character tokens and merge them.\n",
        "* Repeat this process until the predefined iteration limit or the token limit is reached."
      ],
      "metadata": {
        "id": "CXrD8S9y_OjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries\n",
        "\n",
        "re: Provides regular expression matching operations.\n",
        "\n",
        "defaultdict: A subclass of the dictionary that calls a factory function to supply missing values."
      ],
      "metadata": {
        "id": "xMeprWau_1K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "D3gCDVdZ_600"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computes the frequency of consecutive character pairs in the vocabulary."
      ],
      "metadata": {
        "id": "MRYdZBptFvvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(vocab):\n",
        "    \"\"\"\n",
        "    Given a vocabulary (dictionary mapping words to frequency counts), returns a\n",
        "    dictionary of tuples representing the frequency count of pairs of characters\n",
        "    in the vocabulary.\n",
        "    \"\"\"\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "KNJWrQ3nAAC0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merges the most frequent pair of characters in the vocabulary."
      ],
      "metadata": {
        "id": "4du2UNQ7FyTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_vocab(pair, v_in):\n",
        "    \"\"\"\n",
        "    Given a pair of characters and a vocabulary, returns a new vocabulary with the\n",
        "    pair of characters merged together wherever they appear.\n",
        "    \"\"\"\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out"
      ],
      "metadata": {
        "id": "pYw_BRoyFDdF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates a vocabulary with the frequency of each word, where words are split into characters and end with </w>."
      ],
      "metadata": {
        "id": "MQYmLO8sF1XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocab(data):\n",
        "    \"\"\"\n",
        "    Given a list of strings, returns a dictionary of words mapping to their frequency\n",
        "    count in the data.\n",
        "    \"\"\"\n",
        "    vocab = defaultdict(int)\n",
        "    for line in data:\n",
        "        for word in line.split():\n",
        "            vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "zmVfQAaFFFk3"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performs the Byte Pair Encoding algorithm for n iterations."
      ],
      "metadata": {
        "id": "0gWbK64_F3vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def byte_pair_encoding(data, n):\n",
        "    \"\"\"\n",
        "    Given a list of strings and an integer n, returns a list of n merged pairs\n",
        "    of characters found in the vocabulary of the input data.\n",
        "    \"\"\"\n",
        "    vocab = get_vocab(data)\n",
        "    for i in range(n):\n",
        "        pairs = get_stats(vocab)\n",
        "        best = max(pairs, key=pairs.get)\n",
        "        vocab = merge_vocab(best, vocab)\n",
        "    return vocab\n"
      ],
      "metadata": {
        "id": "XA1NDyvwFIXt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "corpus = '''Tokenization is the process of breaking down\n",
        "a sequence of text into smaller units called tokens,\n",
        "which can be words, phrases, or even individual characters.\n",
        "Tokenization is often the first step in natural languages processing tasks\n",
        "such as text classification, named entity recognition, and sentiment analysis.\n",
        "The resulting tokens are typically used as input to further processing steps,\n",
        "such as vectorization, where the tokens are converted\n",
        "into numerical representations for machine learning models to use.'''\n",
        "data = corpus.split('.')\n",
        "\n",
        "n = 230\n",
        "bpe_pairs = byte_pair_encoding(data, n)\n",
        "bpe_pairs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYomeMrkFKfg",
        "outputId": "058fa7a9-581e-422a-cffd-be89c02e67e9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Tokenization</w>': 2,\n",
              " 'is</w>': 2,\n",
              " 'the</w>': 3,\n",
              " 'process</w>': 1,\n",
              " 'of</w>': 2,\n",
              " 'breaking</w>': 1,\n",
              " 'down</w>': 1,\n",
              " 'a</w>': 1,\n",
              " 'sequence</w>': 1,\n",
              " 'text</w>': 2,\n",
              " 'into</w>': 2,\n",
              " 'smaller</w>': 1,\n",
              " 'units</w>': 1,\n",
              " 'called</w>': 1,\n",
              " 'tokens,</w>': 1,\n",
              " 'which</w>': 1,\n",
              " 'can</w>': 1,\n",
              " 'be</w>': 1,\n",
              " 'words,</w>': 1,\n",
              " 'phrases,</w>': 1,\n",
              " 'or</w>': 1,\n",
              " 'even</w>': 1,\n",
              " 'individual</w>': 1,\n",
              " 'characters</w>': 1,\n",
              " 'often</w>': 1,\n",
              " 'first</w>': 1,\n",
              " 'step</w>': 1,\n",
              " 'in</w>': 1,\n",
              " 'natural</w>': 1,\n",
              " 'languages</w>': 1,\n",
              " 'processing</w>': 2,\n",
              " 'tasks</w>': 1,\n",
              " 'such</w>': 2,\n",
              " 'as</w>': 3,\n",
              " 'classification,</w>': 1,\n",
              " 'named</w>': 1,\n",
              " 'entity</w>': 1,\n",
              " 'recognition,</w>': 1,\n",
              " 'and</w>': 1,\n",
              " 'sentiment</w>': 1,\n",
              " 'analysis</w>': 1,\n",
              " 'The</w>': 1,\n",
              " 'resulting</w>': 1,\n",
              " 'tokens</w>': 2,\n",
              " 'are</w>': 2,\n",
              " 'typically</w>': 1,\n",
              " 'used</w>': 1,\n",
              " 'input</w>': 1,\n",
              " 'to</w>': 2,\n",
              " 'further</w>': 1,\n",
              " 'steps,</w>': 1,\n",
              " 'vectorization,</w>': 1,\n",
              " 'where</w>': 1,\n",
              " 'converted</w>': 1,\n",
              " 'numerical</w>': 1,\n",
              " 'representations</w>': 1,\n",
              " 'for</w>': 1,\n",
              " 'machine</w>': 1,\n",
              " 'learning</w>': 1,\n",
              " 'models</w>': 1,\n",
              " 'use</w>': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result:**\n",
        "The output shows the frequency count of merged character pairs identified in the provided text corpus using the Byte Pair Encoding (BPE) algorithm over 230 iterations. Each entry in the resulting dictionary represents a merged character pair and its corresponding frequency within the corpus. The algorithm repeatedly merges the most frequently occurring character pairs until the designated number of iterations is completed."
      ],
      "metadata": {
        "id": "vKrs5J42Gdjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**End of Notebook**"
      ],
      "metadata": {
        "id": "0y_GMhAD-e6I"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}